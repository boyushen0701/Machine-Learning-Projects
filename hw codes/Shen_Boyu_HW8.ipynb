{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>DSCI-552 Homework8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Name: Boyu Shen\n",
    "#### Github username: boyushen0701\n",
    "#### USC ID: 3547352504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import confusion_matrix,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Supervised, Semi-Supervised, and Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>(a) Download the Breast Cancer Wisconsin (Diagnostic) Data Set from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29. \n",
    "<br>Download the data in https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data,\n",
    "<br>which has IDs, classes (Benign=B, Malignant=M), and 30 attributes. This data has two output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>M</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>M</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>M</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>M</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>B</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1      2      3       4       5        6        7        8   \\\n",
       "0      842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('../data/wdbc.data',header=None)\n",
    "data #take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID number</th>\n",
       "      <th>Diagnosis</th>\n",
       "      <th>radius1</th>\n",
       "      <th>texture1</th>\n",
       "      <th>perimeter1</th>\n",
       "      <th>area1</th>\n",
       "      <th>smoothness1</th>\n",
       "      <th>compactness1</th>\n",
       "      <th>concavity1</th>\n",
       "      <th>concave points1</th>\n",
       "      <th>...</th>\n",
       "      <th>radius3</th>\n",
       "      <th>texture3</th>\n",
       "      <th>perimeter3</th>\n",
       "      <th>area3</th>\n",
       "      <th>smoothness3</th>\n",
       "      <th>compactness3</th>\n",
       "      <th>concavity3</th>\n",
       "      <th>concave points3</th>\n",
       "      <th>symmetry3</th>\n",
       "      <th>fractal dimension3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>1</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>1</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>1</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>1</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>0</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID number  Diagnosis  radius1  texture1  perimeter1   area1  smoothness1  \\\n",
       "0       842302          1    17.99     10.38      122.80  1001.0      0.11840   \n",
       "1       842517          1    20.57     17.77      132.90  1326.0      0.08474   \n",
       "2     84300903          1    19.69     21.25      130.00  1203.0      0.10960   \n",
       "3     84348301          1    11.42     20.38       77.58   386.1      0.14250   \n",
       "4     84358402          1    20.29     14.34      135.10  1297.0      0.10030   \n",
       "..         ...        ...      ...       ...         ...     ...          ...   \n",
       "564     926424          1    21.56     22.39      142.00  1479.0      0.11100   \n",
       "565     926682          1    20.13     28.25      131.20  1261.0      0.09780   \n",
       "566     926954          1    16.60     28.08      108.30   858.1      0.08455   \n",
       "567     927241          1    20.60     29.33      140.10  1265.0      0.11780   \n",
       "568      92751          0     7.76     24.54       47.92   181.0      0.05263   \n",
       "\n",
       "     compactness1  concavity1  concave points1  ...  radius3  texture3  \\\n",
       "0         0.27760     0.30010          0.14710  ...   25.380     17.33   \n",
       "1         0.07864     0.08690          0.07017  ...   24.990     23.41   \n",
       "2         0.15990     0.19740          0.12790  ...   23.570     25.53   \n",
       "3         0.28390     0.24140          0.10520  ...   14.910     26.50   \n",
       "4         0.13280     0.19800          0.10430  ...   22.540     16.67   \n",
       "..            ...         ...              ...  ...      ...       ...   \n",
       "564       0.11590     0.24390          0.13890  ...   25.450     26.40   \n",
       "565       0.10340     0.14400          0.09791  ...   23.690     38.25   \n",
       "566       0.10230     0.09251          0.05302  ...   18.980     34.12   \n",
       "567       0.27700     0.35140          0.15200  ...   25.740     39.42   \n",
       "568       0.04362     0.00000          0.00000  ...    9.456     30.37   \n",
       "\n",
       "     perimeter3   area3  smoothness3  compactness3  concavity3  \\\n",
       "0        184.60  2019.0      0.16220       0.66560      0.7119   \n",
       "1        158.80  1956.0      0.12380       0.18660      0.2416   \n",
       "2        152.50  1709.0      0.14440       0.42450      0.4504   \n",
       "3         98.87   567.7      0.20980       0.86630      0.6869   \n",
       "4        152.20  1575.0      0.13740       0.20500      0.4000   \n",
       "..          ...     ...          ...           ...         ...   \n",
       "564      166.10  2027.0      0.14100       0.21130      0.4107   \n",
       "565      155.00  1731.0      0.11660       0.19220      0.3215   \n",
       "566      126.70  1124.0      0.11390       0.30940      0.3403   \n",
       "567      184.60  1821.0      0.16500       0.86810      0.9387   \n",
       "568       59.16   268.6      0.08996       0.06444      0.0000   \n",
       "\n",
       "     concave points3  symmetry3  fractal dimension3  \n",
       "0             0.2654     0.4601             0.11890  \n",
       "1             0.1860     0.2750             0.08902  \n",
       "2             0.2430     0.3613             0.08758  \n",
       "3             0.2575     0.6638             0.17300  \n",
       "4             0.1625     0.2364             0.07678  \n",
       "..               ...        ...                 ...  \n",
       "564           0.2216     0.2060             0.07115  \n",
       "565           0.1628     0.2572             0.06637  \n",
       "566           0.1418     0.2218             0.07820  \n",
       "567           0.2650     0.4087             0.12400  \n",
       "568           0.0000     0.2871             0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=[]\n",
    "get_name=False\n",
    "for line in open('../data/wdbc.names'):\n",
    "    line=line.strip()\n",
    "    sp=line.split(' ')\n",
    "    if 'Attribute' in line:\n",
    "        get_name=True\n",
    "    if 'Several' in line:\n",
    "        get_name=False\n",
    "    if get_name==True:\n",
    "        try:\n",
    "            columns.append(sp[1])\n",
    "        except:\n",
    "            None\n",
    "\n",
    "columns[1]='ID number'\n",
    "columns[-3]='concave points'\n",
    "columns[-1]='fractal dimension'\n",
    "columns.remove('Attribute')\n",
    "columns.remove('real-valued')\n",
    "attrs=[]\n",
    "for j in [1,2,3]:\n",
    "    for attr in columns[2:]:\n",
    "        attrs.append(attr+str(j))\n",
    "names=columns[:2]\n",
    "names.extend(attrs) # create the column names for the dataframe\n",
    "data.columns=names\n",
    "data.replace('M', 1, inplace=True) # mark Malignant as positive class(1)\n",
    "data.replace('B', 0, inplace=True) # mark Benign as negative class(0)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Monte-Carlo Simulation: Repeat the following procedures for supervised, un-supervised, and semi-supervised learning M = 30 times, and \n",
    "use randomly selected train and test data (make sure you use 20% of both the positve and negative classes as the test set). Then compare the \n",
    "average scores (accuracy, precision, recall, F1-score, and AUC) that you obtain from each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Supervised Learning: Train an L1-penalized SVM to classify the data. Use 5 fold cross validation to choose the penalty parameter. Use \n",
    "normalized data. Report the average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. Plot \n",
    "the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1 completed, the best c is 1\n",
      "Simulation 2 completed, the best c is 10\n",
      "Simulation 3 completed, the best c is 1\n",
      "Simulation 4 completed, the best c is 1\n",
      "Simulation 5 completed, the best c is 1\n",
      "Simulation 6 completed, the best c is 1\n",
      "Simulation 7 completed, the best c is 10\n",
      "Simulation 8 completed, the best c is 1\n",
      "Simulation 9 completed, the best c is 1\n",
      "Simulation 10 completed, the best c is 1\n",
      "Simulation 11 completed, the best c is 1\n",
      "Simulation 12 completed, the best c is 1\n",
      "Simulation 13 completed, the best c is 1\n",
      "Simulation 14 completed, the best c is 1\n",
      "Simulation 15 completed, the best c is 10\n",
      "Simulation 16 completed, the best c is 1\n",
      "Simulation 17 completed, the best c is 1\n",
      "Simulation 18 completed, the best c is 10\n",
      "Simulation 19 completed, the best c is 10\n",
      "Simulation 20 completed, the best c is 10\n",
      "Simulation 21 completed, the best c is 1\n",
      "Simulation 22 completed, the best c is 1\n",
      "Simulation 23 completed, the best c is 1\n",
      "Simulation 24 completed, the best c is 10\n",
      "Simulation 25 completed, the best c is 1\n",
      "Simulation 26 completed, the best c is 1\n",
      "Simulation 27 completed, the best c is 1\n",
      "Simulation 28 completed, the best c is 1\n",
      "Simulation 29 completed, the best c is 1\n",
      "Simulation 30 completed, the best c is 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "M=30\n",
    "Cs=[]\n",
    "X_set=data.iloc[:,2:]\n",
    "Y_set=data.iloc[:,1]\n",
    "log_c=range(-3,5)\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_precision=[]\n",
    "test_precision=[]\n",
    "train_recall=[]\n",
    "test_recall=[]\n",
    "train_f1=[]\n",
    "test_f1=[]\n",
    "train_auc=[]\n",
    "test_auc=[]\n",
    "for l in log_c:\n",
    "    Cs.append(10**l)\n",
    "for i in range(M):\n",
    "    X_train, X_test, Y_train, Y_test=train_test_split(X_set,Y_set,test_size=0.2,random_state=i,stratify=Y_set) # Monte-Carlo Simulation train test split\n",
    "    MMS=MinMaxScaler() # normalized data\n",
    "    MMS.fit(X_train)\n",
    "    X_train_norm=MMS.transform(X_train)\n",
    "    X_test_norm=MMS.transform(X_test)\n",
    "    svc=LinearSVC(random_state=22, penalty='l1',dual=False,max_iter=1000000)\n",
    "    parameters={'C':Cs}\n",
    "    kfold=StratifiedKFold(n_splits=5)\n",
    "    GS1=GridSearchCV(svc, parameters, cv=kfold ,n_jobs=-1).fit(X_train_norm, Y_train)\n",
    "    c1=GS1.best_params_['C'] # best c from cross validation\n",
    "    lsvc=LinearSVC(random_state=22, penalty='l1',dual=False,max_iter=1000000,C=c1) # retrain the model on the whole training set\n",
    "    lsvc.fit(X_train_norm,Y_train)\n",
    "    train_accuracy.append(lsvc.score(X_train_norm,Y_train))\n",
    "    test_accuracy.append(lsvc.score(X_test_norm,Y_test))\n",
    "    train_pred=lsvc.predict(X_train_norm)\n",
    "    test_pred=lsvc.predict(X_test_norm)\n",
    "    train_precision.append(precision_score(Y_train,train_pred))\n",
    "    test_precision.append(precision_score(Y_test,test_pred))\n",
    "    train_recall.append(recall_score(Y_train,train_pred))\n",
    "    test_recall.append(recall_score(Y_test,test_pred))\n",
    "    train_f1.append(f1_score(Y_train,train_pred))\n",
    "    test_f1.append(f1_score(Y_test,test_pred))\n",
    "    train_prob=lsvc.decision_function(X_train_norm)\n",
    "    test_prob=lsvc.decision_function(X_test_norm)\n",
    "    train_auc.append(roc_auc_score(Y_train,train_prob))\n",
    "    test_auc.append(roc_auc_score(Y_test,test_prob))\n",
    "    print('Simulation',i+1,'completed, the best c is',c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average statistics over 30 simulations:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training set</th>\n",
       "      <td>0.984982</td>\n",
       "      <td>0.989023</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>0.979697</td>\n",
       "      <td>0.997510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test set</th>\n",
       "      <td>0.965497</td>\n",
       "      <td>0.971672</td>\n",
       "      <td>0.934127</td>\n",
       "      <td>0.952108</td>\n",
       "      <td>0.992758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              accuracy  precision    recall  F1-score       AUC\n",
       "training set  0.984982   0.989023  0.970588  0.979697  0.997510\n",
       "test set      0.965497   0.971672  0.934127  0.952108  0.992758"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_train_accuracy=sum(train_accuracy)/len(train_accuracy)\n",
    "avg_test_accuracy=sum(test_accuracy)/len(test_accuracy)\n",
    "avg_train_precision=sum(train_precision)/len(train_precision)\n",
    "avg_test_precision=sum(test_precision)/len(test_precision)\n",
    "avg_train_recall=sum(train_recall)/len(train_recall)\n",
    "avg_test_recall=sum(test_recall)/len(test_recall)\n",
    "avg_train_f1=sum(train_f1)/len(train_f1)\n",
    "avg_test_f1=sum(test_f1)/len(test_f1)\n",
    "avg_train_auc=sum(train_auc)/len(train_auc)\n",
    "avg_test_auc=sum(test_auc)/len(test_auc)\n",
    "stats=[[avg_train_accuracy,avg_train_precision,avg_train_recall,avg_train_f1,avg_train_auc],[avg_test_accuracy,avg_test_precision,avg_test_recall,avg_test_f1,avg_test_auc]]\n",
    "report=pd.DataFrame(data=stats,index=['training set','test set'],columns=['accuracy','precision','recall','F1-score','AUC'])\n",
    "print('The average statistics over 30 simulations:')\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for the last simulation, report the ROC and the confusion matrix\n",
    "train_fpr,train_tpr,train_thresholds=roc_curve(Y_train,train_prob)\n",
    "test_fpr,test_tpr,test_thresholds=roc_curve(Y_test,test_prob)\n",
    "plt.figure()\n",
    "plt.plot(train_fpr,train_tpr,label='Training ROC curve')\n",
    "plt.plot(test_fpr,test_tpr,label='Testing ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC for L1-penalized SVM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for training set:\n",
      "[[283   2]\n",
      " [  7 163]]\n",
      "Confusion matrix for testing set:\n",
      "[[72  0]\n",
      " [ 1 41]]\n"
     ]
    }
   ],
   "source": [
    "train_cfm=confusion_matrix(Y_train,train_pred)\n",
    "test_cfm=confusion_matrix(Y_test,test_pred)\n",
    "print('Confusion matrix for training set:')\n",
    "print(train_cfm)\n",
    "print('Confusion matrix for testing set:')\n",
    "print(test_cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Semi-Supervised Learning/ Self-training: select 50% of the positive class along with 50% of the negative class in the training set as \n",
    "labeled data and the rest as unlabelled data. You can select them randomly.\n",
    "<br>A. Train an L1-penalized SVM to classify the labeled data Use normalized data. Choose the penalty parameter using 5 fold cross validation.\n",
    "<br>B. Find the unlabeled data point that is the farthest to the decision boundary of the SVM. Let the SVM label it (ignore its true label), and add it to the labeled data, and retrain the SVM. Continue this process until all unlabeled data are used. Test the final SVM on the test data andthe average accuracy, precision, recall, F1-score, and AUC, for both training and test sets over your M runs. Plot the ROC and report the confusion matrix for training and testing in one of the runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1 completed, the final best c is 1\n",
      "Simulation 2 completed, the final best c is 1\n",
      "Simulation 3 completed, the final best c is 1\n",
      "Simulation 4 completed, the final best c is 10\n",
      "Simulation 5 completed, the final best c is 1\n",
      "Simulation 6 completed, the final best c is 1\n",
      "Simulation 7 completed, the final best c is 10\n",
      "Simulation 8 completed, the final best c is 10\n",
      "Simulation 9 completed, the final best c is 1\n",
      "Simulation 10 completed, the final best c is 100\n",
      "Simulation 11 completed, the final best c is 1\n",
      "Simulation 12 completed, the final best c is 1\n",
      "Simulation 13 completed, the final best c is 1\n",
      "Simulation 14 completed, the final best c is 1\n",
      "Simulation 15 completed, the final best c is 1\n",
      "Simulation 16 completed, the final best c is 1\n",
      "Simulation 17 completed, the final best c is 10\n",
      "Simulation 18 completed, the final best c is 10\n",
      "Simulation 19 completed, the final best c is 1\n",
      "Simulation 20 completed, the final best c is 1000\n",
      "Simulation 21 completed, the final best c is 1\n",
      "Simulation 22 completed, the final best c is 1\n",
      "Simulation 23 completed, the final best c is 10\n",
      "Simulation 24 completed, the final best c is 1\n",
      "Simulation 25 completed, the final best c is 1\n",
      "Simulation 26 completed, the final best c is 1\n",
      "Simulation 27 completed, the final best c is 10\n",
      "Simulation 28 completed, the final best c is 1\n",
      "Simulation 29 completed, the final best c is 10\n",
      "Simulation 30 completed, the final best c is 1\n"
     ]
    }
   ],
   "source": [
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_precision=[]\n",
    "test_precision=[]\n",
    "train_recall=[]\n",
    "test_recall=[]\n",
    "train_f1=[]\n",
    "test_f1=[]\n",
    "train_auc=[]\n",
    "test_auc=[]\n",
    "for i in range(M):\n",
    "    X_train, X_test, Y_train, Y_test=train_test_split(X_set,Y_set,test_size=0.2,random_state=i,stratify=Y_set) # Monte-Carlo Simulation train test split\n",
    "    MMS=MinMaxScaler() # normalized data\n",
    "    MMS.fit(X_train)\n",
    "    X_train_norm=MMS.transform(X_train)\n",
    "    X_test_norm=MMS.transform(X_test)\n",
    "    X_label, X_unlabel, Y_label, Y_unlabel=train_test_split(X_train_norm,Y_train,test_size=0.5,random_state=i,stratify=Y_train)\n",
    "    X_label=pd.DataFrame(X_label)\n",
    "    X_unlabel=pd.DataFrame(X_unlabel)\n",
    "    svc=LinearSVC(random_state=22, penalty='l1',dual=False,max_iter=1000000)\n",
    "    parameters={'C':Cs}\n",
    "    kfold=StratifiedKFold(n_splits=5)\n",
    "    GS2=GridSearchCV(svc, parameters, cv=kfold ,n_jobs=-1).fit(X_label, Y_label)\n",
    "    c2=GS2.best_params_['C'] # best c from cross validation\n",
    "    lsvc=LinearSVC(random_state=22, penalty='l1',dual=False,max_iter=1000000,C=c2)\n",
    "    lsvc.fit(X_label,Y_label)\n",
    "    x_train=X_label\n",
    "    y_train=Y_label\n",
    "    while len(X_unlabel)>0:\n",
    "        distances=np.abs(lsvc.decision_function(X_unlabel))\n",
    "        pos=distances.argmax()\n",
    "        if lsvc.decision_function(X_unlabel)[pos]<0:\n",
    "            label=0\n",
    "        else:\n",
    "            label=1\n",
    "        x_train=np.append(x_train,[X_unlabel.iloc[pos]],axis=0)\n",
    "        y_train=y_train.append(pd.Series(label))\n",
    "        X_unlabel=X_unlabel.drop(X_unlabel.index[pos]) # remove the farthest unlabeled data\n",
    "        lsvc.fit(x_train,y_train)\n",
    "    kfold=StratifiedKFold(n_splits=5)\n",
    "    GS=GridSearchCV(lsvc, parameters, cv=kfold ,n_jobs=-1).fit(x_train,y_train)\n",
    "    c=GS.best_params_['C'] # best c from cross validation\n",
    "    lsvc=LinearSVC(random_state=22, penalty='l1',dual=False,max_iter=1000000,C=c)\n",
    "    lsvc.fit(x_train,y_train) # retrain the svm\n",
    "    train_accuracy.append(lsvc.score(X_train_norm,Y_train))\n",
    "    test_accuracy.append(lsvc.score(X_test_norm,Y_test))\n",
    "    train_pred=lsvc.predict(X_train_norm)\n",
    "    test_pred=lsvc.predict(X_test_norm)\n",
    "    train_precision.append(precision_score(Y_train,train_pred))\n",
    "    test_precision.append(precision_score(Y_test,test_pred))\n",
    "    train_recall.append(recall_score(Y_train,train_pred))\n",
    "    test_recall.append(recall_score(Y_test,test_pred))\n",
    "    train_f1.append(f1_score(Y_train,train_pred))\n",
    "    test_f1.append(f1_score(Y_test,test_pred))\n",
    "    train_prob=lsvc.decision_function(X_train_norm)\n",
    "    test_prob=lsvc.decision_function(X_test_norm)\n",
    "    train_auc.append(roc_auc_score(Y_train,train_prob))\n",
    "    test_auc.append(roc_auc_score(Y_test,test_prob))\n",
    "    print('Simulation',i+1,'completed, the final best c is',c)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average statistics over 30 simulations for Semi-Supervised Learning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training set</th>\n",
       "      <td>0.976190</td>\n",
       "      <td>0.983433</td>\n",
       "      <td>0.952549</td>\n",
       "      <td>0.967587</td>\n",
       "      <td>0.994125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test set</th>\n",
       "      <td>0.960819</td>\n",
       "      <td>0.970183</td>\n",
       "      <td>0.923016</td>\n",
       "      <td>0.945413</td>\n",
       "      <td>0.991733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              accuracy  precision    recall  F1-score       AUC\n",
       "training set  0.976190   0.983433  0.952549  0.967587  0.994125\n",
       "test set      0.960819   0.970183  0.923016  0.945413  0.991733"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_train_accuracy=sum(train_accuracy)/len(train_accuracy)\n",
    "avg_test_accuracy=sum(test_accuracy)/len(test_accuracy)\n",
    "avg_train_precision=sum(train_precision)/len(train_precision)\n",
    "avg_test_precision=sum(test_precision)/len(test_precision)\n",
    "avg_train_recall=sum(train_recall)/len(train_recall)\n",
    "avg_test_recall=sum(test_recall)/len(test_recall)\n",
    "avg_train_f1=sum(train_f1)/len(train_f1)\n",
    "avg_test_f1=sum(test_f1)/len(test_f1)\n",
    "avg_train_auc=sum(train_auc)/len(train_auc)\n",
    "avg_test_auc=sum(test_auc)/len(test_auc)\n",
    "stats1=[[avg_train_accuracy,avg_train_precision,avg_train_recall,avg_train_f1,avg_train_auc],[avg_test_accuracy,avg_test_precision,avg_test_recall,avg_test_f1,avg_test_auc]]\n",
    "report1=pd.DataFrame(data=stats1,index=['training set','test set'],columns=['accuracy','precision','recall','F1-score','AUC'])\n",
    "print('The average statistics over 30 simulations for Semi-Supervised Learning:')\n",
    "report1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for the last simulation, report the ROC and the confusion matrix\n",
    "train_fpr,train_tpr,train_thresholds=roc_curve(Y_train,train_prob)\n",
    "test_fpr,test_tpr,test_thresholds=roc_curve(Y_test,test_prob)\n",
    "plt.figure()\n",
    "plt.plot(train_fpr,train_tpr,label='Training ROC curve')\n",
    "plt.plot(test_fpr,test_tpr,label='Testing ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC for L1-penalized SVM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for training set:\n",
      "[[276   9]\n",
      " [ 10 160]]\n",
      "Confusion matrix for testing set:\n",
      "[[70  2]\n",
      " [ 2 40]]\n"
     ]
    }
   ],
   "source": [
    "train_cfm=confusion_matrix(Y_train,train_pred)\n",
    "test_cfm=confusion_matrix(Y_test,test_pred)\n",
    "print('Confusion matrix for training set:')\n",
    "print(train_cfm)\n",
    "print('Confusion matrix for testing set:')\n",
    "print(test_cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Unsupervised Learning: Run k-means algorithm on the whole training set. Ignore the labels of the data, and assume k = 2.\n",
    "<br>A. Run the k-means algorithm multiple times. Make sure that you initialize the algorithm randomly. How do you make sure that the algorithm \n",
    "was not trapped in a local minimum?\n",
    "<br>B. Compute the centers of the two clusters and find the closest 30 data points to each center. Read the true labels of those 30 data points and take a majority poll within them. The majority poll becomes the label predicted by k-means for the members of each cluster. Then compare the labels provided by k-means with the true labels of the training data and report the average accuracy, precision, recall, F1-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs.\n",
    "<br>C. Classify test data based on their proximity to the centers of the clusters.\n",
    "Report the average accuracy, precision, recall, F1-score, and AUC over M runs, and ROC and the confusion matrix for one of the runs for the\n",
    "test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to debug the n_init parameter larger in KMeans algorithm so that the best cluster result will be selected from various intial \n",
    "situations. Hence, we can avoid being trapped in a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1 completed\n",
      "Simulation 2 completed\n",
      "Simulation 3 completed\n",
      "Simulation 4 completed\n",
      "Simulation 5 completed\n",
      "Simulation 6 completed\n",
      "Simulation 7 completed\n",
      "Simulation 8 completed\n",
      "Simulation 9 completed\n",
      "Simulation 10 completed\n",
      "Simulation 11 completed\n",
      "Simulation 12 completed\n",
      "Simulation 13 completed\n",
      "Simulation 14 completed\n",
      "Simulation 15 completed\n",
      "Simulation 16 completed\n",
      "Simulation 17 completed\n",
      "Simulation 18 completed\n",
      "Simulation 19 completed\n",
      "Simulation 20 completed\n",
      "Simulation 21 completed\n",
      "Simulation 22 completed\n",
      "Simulation 23 completed\n",
      "Simulation 24 completed\n",
      "Simulation 25 completed\n",
      "Simulation 26 completed\n",
      "Simulation 27 completed\n",
      "Simulation 28 completed\n",
      "Simulation 29 completed\n",
      "Simulation 30 completed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.special import softmax\n",
    "\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_precision=[]\n",
    "test_precision=[]\n",
    "train_recall=[]\n",
    "test_recall=[]\n",
    "train_f1=[]\n",
    "test_f1=[]\n",
    "train_auc=[]\n",
    "test_auc=[]\n",
    "for i in range(M):\n",
    "    X_train, X_test, Y_train, Y_test=train_test_split(X_set,Y_set,test_size=0.2,random_state=i,stratify=Y_set) # Monte-Carlo Simulation train test split\n",
    "    cluster=KMeans(random_state=22,n_clusters=2,n_init=50)\n",
    "    cluster.fit(X_train)\n",
    "    labels=cluster.labels_\n",
    "    centers=cluster.cluster_centers_\n",
    "    index_0=np.where(labels==0)\n",
    "    index_1=np.where(labels==1)\n",
    "    cluster_0=X_train.iloc[index_0]\n",
    "    cluster_1=X_train.iloc[index_1]\n",
    "    distances0={}\n",
    "    distances1={}\n",
    "    order0=[]\n",
    "    order1=[]\n",
    "    for k in range(len(cluster_0)):\n",
    "        distances0[index_0[0][k]]=np.linalg.norm(cluster_0.iloc[k]-centers[0]) # key is the order in X_train, value is the distance\n",
    "    for l in range(len(cluster_1)):\n",
    "        distances1[index_1[0][l]]=np.linalg.norm(cluster_1.iloc[l]-centers[1])\n",
    "    s1=sorted([(v1,k1) for k1,v1 in distances0.items()],reverse=False)\n",
    "    s2=sorted([(v2,k2) for k2,v2 in distances1.items()],reverse=False)\n",
    "    for m in range(30):\n",
    "        order0.append(s1[m][1]) # the orders in X_train of the 30 cloest points\n",
    "        order1.append(s2[m][1])\n",
    "    labels0=[]\n",
    "    labels1=[]\n",
    "    for p in order0:\n",
    "        labels0.append(list(Y_train)[p])\n",
    "    for q in order1:\n",
    "        labels1.append(list(Y_train)[q])\n",
    "    if labels0.count(0)>labels0.count(1):\n",
    "        label0=0\n",
    "    else:\n",
    "        label0=1\n",
    "    if labels1.count(0)>labels1.count(1):\n",
    "        label1=0\n",
    "    else:\n",
    "        label1=1\n",
    "    train_pred=[]\n",
    "    test_pred=[]\n",
    "    for t in range(len(Y_train)):\n",
    "        if t in index_0[0]:\n",
    "            train_pred.append(label0)\n",
    "        else:\n",
    "            train_pred.append(label1)\n",
    "    for s in range(len(Y_test)): # Classify test data based on their proximity to the centers of the clusters\n",
    "        if np.linalg.norm(X_test.iloc[s]-centers[0])<np.linalg.norm(X_test.iloc[s]-centers[1]): \n",
    "            test_pred.append(label0)\n",
    "        else:\n",
    "            test_pred.append(label1)\n",
    "    distances_train=[]\n",
    "    distances_test=[]\n",
    "    for u in range(len(X_train)):\n",
    "        distances_train.append([-np.linalg.norm(X_train.iloc[u]-centers[0]),-np.linalg.norm(X_train.iloc[u]-centers[1])])\n",
    "    for v in range(len(X_test)):\n",
    "        distances_test.append([-np.linalg.norm(X_test.iloc[v]-centers[0]),-np.linalg.norm(X_test.iloc[v]-centers[1])])\n",
    "    train_prob=softmax(distances_train,axis=1)[:,label1] # use softmax to calculate probabilities\n",
    "    test_prob=softmax(distances_test,axis=1)[:,label1] # if label0 is 0, probability of class1 is in proportion to distance(negative) to center of cluster 1\n",
    "    train_accuracy.append(accuracy_score(Y_train,train_pred))\n",
    "    test_accuracy.append(accuracy_score(Y_test,test_pred))\n",
    "    train_precision.append(precision_score(Y_train,train_pred))\n",
    "    test_precision.append(precision_score(Y_test,test_pred))\n",
    "    train_recall.append(recall_score(Y_train,train_pred))\n",
    "    test_recall.append(recall_score(Y_test,test_pred))\n",
    "    train_f1.append(f1_score(Y_train,train_pred))\n",
    "    test_f1.append(f1_score(Y_test,test_pred))\n",
    "    train_auc.append(roc_auc_score(Y_train,train_prob))\n",
    "    test_auc.append(roc_auc_score(Y_test,test_prob))\n",
    "    print('Simulation',i+1,'completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average statistics over 30 simulations for Unsupervised Learning:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training set</th>\n",
       "      <td>0.849817</td>\n",
       "      <td>0.991316</td>\n",
       "      <td>0.603333</td>\n",
       "      <td>0.749817</td>\n",
       "      <td>0.900263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test set</th>\n",
       "      <td>0.854094</td>\n",
       "      <td>0.996235</td>\n",
       "      <td>0.606349</td>\n",
       "      <td>0.752364</td>\n",
       "      <td>0.902397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              accuracy  precision    recall  F1-score       AUC\n",
       "training set  0.849817   0.991316  0.603333  0.749817  0.900263\n",
       "test set      0.854094   0.996235  0.606349  0.752364  0.902397"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_train_accuracy=sum(train_accuracy)/len(train_accuracy)\n",
    "avg_test_accuracy=sum(test_accuracy)/len(test_accuracy)\n",
    "avg_train_precision=sum(train_precision)/len(train_precision)\n",
    "avg_test_precision=sum(test_precision)/len(test_precision)\n",
    "avg_train_recall=sum(train_recall)/len(train_recall)\n",
    "avg_test_recall=sum(test_recall)/len(test_recall)\n",
    "avg_train_f1=sum(train_f1)/len(train_f1)\n",
    "avg_test_f1=sum(test_f1)/len(test_f1)\n",
    "avg_train_auc=sum(train_auc)/len(train_auc)\n",
    "avg_test_auc=sum(test_auc)/len(test_auc)\n",
    "stats2=[[avg_train_accuracy,avg_train_precision,avg_train_recall,avg_train_f1,avg_train_auc],[avg_test_accuracy,avg_test_precision,avg_test_recall,avg_test_f1,avg_test_auc]]\n",
    "report2=pd.DataFrame(data=stats2,index=['training set','test set'],columns=['accuracy','precision','recall','F1-score','AUC'])\n",
    "print('The average statistics over 30 simulations for Unsupervised Learning:')\n",
    "report2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for the last simulation, report the ROC and the confusion matrix\n",
    "train_fpr,train_tpr,train_thresholds=roc_curve(Y_train,train_prob)\n",
    "test_fpr,test_tpr,test_thresholds=roc_curve(Y_test,test_prob)\n",
    "plt.figure()\n",
    "plt.plot(train_fpr,train_tpr,label='Training ROC curve')\n",
    "plt.plot(test_fpr,test_tpr,label='Testing ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC for k-means')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for training set:\n",
      "[[284   1]\n",
      " [ 71  99]]\n",
      "Confusion matrix for testing set:\n",
      "[[72  0]\n",
      " [12 30]]\n"
     ]
    }
   ],
   "source": [
    "train_cfm=confusion_matrix(Y_train,train_pred)\n",
    "test_cfm=confusion_matrix(Y_test,test_pred)\n",
    "print('Confusion matrix for training set:')\n",
    "print(train_cfm)\n",
    "print('Confusion matrix for testing set:')\n",
    "print(test_cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Spectral Clustering: Repeat 1(b)iii using spectral clustering, which is clustering based on kernels. Research what spectral clustering is.\n",
    "Use RBF kernel with gamma=1 or find a gamma for which the two clutsres have the same balance as the one in original data set (if the positive \n",
    "class has p and the negative class has n samples, the two clusters must have p and n members). Do not label data based on their proximity \n",
    "to cluster center, because spectral clustering may give you non-convex clusters . Instead, use fit - predict method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multivariate statistics, spectral clustering techniques make use of the spectrum (eigenvalues) of the similarity matrix of the data to perform dimensionality reduction before clustering in fewer dimensions. The similarity matrix is provided as an input and consists of a quantitative assessment of the relative similarity of each pair of points in the dataset.\n",
    "Cited from: https://en.wikipedia.org/wiki/Spectral_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation 1 completed\n",
      "Simulation 2 completed\n",
      "Simulation 3 completed\n",
      "Simulation 4 completed\n",
      "Simulation 5 completed\n",
      "Simulation 6 completed\n",
      "Simulation 7 completed\n",
      "Simulation 8 completed\n",
      "Simulation 9 completed\n",
      "Simulation 10 completed\n",
      "Simulation 11 completed\n",
      "Simulation 12 completed\n",
      "Simulation 13 completed\n",
      "Simulation 14 completed\n",
      "Simulation 15 completed\n",
      "Simulation 16 completed\n",
      "Simulation 17 completed\n",
      "Simulation 18 completed\n",
      "Simulation 19 completed\n",
      "Simulation 20 completed\n",
      "Simulation 21 completed\n",
      "Simulation 22 completed\n",
      "Simulation 23 completed\n",
      "Simulation 24 completed\n",
      "Simulation 25 completed\n",
      "Simulation 26 completed\n",
      "Simulation 27 completed\n",
      "Simulation 28 completed\n",
      "Simulation 29 completed\n",
      "Simulation 30 completed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "M=30\n",
    "X_set=data.iloc[:,2:]\n",
    "Y_set=data.iloc[:,1]\n",
    "train_accuracy=[]\n",
    "test_accuracy=[]\n",
    "train_precision=[]\n",
    "test_precision=[]\n",
    "train_recall=[]\n",
    "test_recall=[]\n",
    "train_f1=[]\n",
    "test_f1=[]\n",
    "train_auc=[]\n",
    "test_auc=[]\n",
    "for i in range(M):\n",
    "    X_train, X_test, Y_train, Y_test=train_test_split(X_set,Y_set,test_size=0.2,random_state=i,stratify=Y_set) # Monte-Carlo Simulation train test split\n",
    "    MMS=MinMaxScaler() # normalized data\n",
    "    MMS.fit(X_train)\n",
    "    X_train_norm=MMS.transform(X_train)\n",
    "    X_test_norm=MMS.transform(X_test)\n",
    "    spclusters=SpectralClustering(n_clusters=2,random_state=22,gamma=1,affinity='rbf')\n",
    "    # for training set\n",
    "    cluster_result_train=spclusters.fit_predict(X_train_norm)\n",
    "    index_0_train=[]\n",
    "    index_1_train=[]\n",
    "    for j in range(len(cluster_result_train)):\n",
    "        if cluster_result_train[j]==0:\n",
    "            index_0_train.append(j)\n",
    "        else:\n",
    "            index_1_train.append(j)\n",
    "    labels0_train=[]\n",
    "    labels1_train=[]\n",
    "    for p in index_0_train:\n",
    "        labels0_train.append(list(Y_train)[p])\n",
    "    for q in index_1_train:\n",
    "        labels1_train.append(list(Y_train)[q])\n",
    "    if labels0_train.count(0)>labels0_train.count(1):\n",
    "        label0_train=0\n",
    "    else:\n",
    "        label0_train=1\n",
    "    if labels1_train.count(0)>labels1_train.count(1):\n",
    "        label1_train=0\n",
    "    else:\n",
    "        label1_train=1\n",
    "    train_pred=[]\n",
    "    for t in range(len(Y_train)):\n",
    "        if t in index_0_train:\n",
    "            train_pred.append(label0_train)\n",
    "        else:\n",
    "            train_pred.append(label1_train)\n",
    "    # for test set\n",
    "    cluster_result_test=spclusters.fit_predict(X_test_norm)\n",
    "    index_0_test=[]\n",
    "    index_1_test=[]\n",
    "    for k in range(len(cluster_result_test)):\n",
    "        if cluster_result_test[k]==0:\n",
    "            index_0_test.append(k)\n",
    "        else:\n",
    "            index_1_test.append(k)\n",
    "    labels0_test=[]\n",
    "    labels1_test=[]\n",
    "    for p in index_0_test:\n",
    "        labels0_test.append(list(Y_test)[p])\n",
    "    for q in index_1_test:\n",
    "        labels1_test.append(list(Y_test)[q])\n",
    "    if labels0_test.count(0)>labels0_test.count(1):\n",
    "        label0_test=0\n",
    "    else:\n",
    "        label0_test=1\n",
    "    if labels1_test.count(0)>labels1_test.count(1):\n",
    "        label1_test=0\n",
    "    else:\n",
    "        label1_test=1\n",
    "    test_pred=[]\n",
    "    for z in range(len(Y_test)):\n",
    "        if z in index_0_test:\n",
    "            test_pred.append(label0_test)\n",
    "        else:\n",
    "            test_pred.append(label1_test)\n",
    "    train_accuracy.append(accuracy_score(Y_train,train_pred))\n",
    "    test_accuracy.append(accuracy_score(Y_test,test_pred))\n",
    "    train_precision.append(precision_score(Y_train,train_pred))\n",
    "    test_precision.append(precision_score(Y_test,test_pred))\n",
    "    train_recall.append(recall_score(Y_train,train_pred))\n",
    "    test_recall.append(recall_score(Y_test,test_pred))\n",
    "    train_f1.append(f1_score(Y_train,train_pred))\n",
    "    test_f1.append(f1_score(Y_test,test_pred))\n",
    "    train_auc.append(roc_auc_score(Y_train,train_pred))\n",
    "    test_auc.append(roc_auc_score(Y_test,test_pred))\n",
    "    print('Simulation',i+1,'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average statistics over 30 simulations for Spectral Clustering:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training set</th>\n",
       "      <td>0.862344</td>\n",
       "      <td>0.980199</td>\n",
       "      <td>0.644706</td>\n",
       "      <td>0.776668</td>\n",
       "      <td>0.818435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test set</th>\n",
       "      <td>0.854386</td>\n",
       "      <td>0.988086</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.741529</td>\n",
       "      <td>0.804365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              accuracy  precision    recall  F1-score       AUC\n",
       "training set  0.862344   0.980199  0.644706  0.776668  0.818435\n",
       "test set      0.854386   0.988086  0.614286  0.741529  0.804365"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_train_accuracy=sum(train_accuracy)/len(train_accuracy)\n",
    "avg_test_accuracy=sum(test_accuracy)/len(test_accuracy)\n",
    "avg_train_precision=sum(train_precision)/len(train_precision)\n",
    "avg_test_precision=sum(test_precision)/len(test_precision)\n",
    "avg_train_recall=sum(train_recall)/len(train_recall)\n",
    "avg_test_recall=sum(test_recall)/len(test_recall)\n",
    "avg_train_f1=sum(train_f1)/len(train_f1)\n",
    "avg_test_f1=sum(test_f1)/len(test_f1)\n",
    "avg_train_auc=sum(train_auc)/len(train_auc)\n",
    "avg_test_auc=sum(test_auc)/len(test_auc)\n",
    "stats3=[[avg_train_accuracy,avg_train_precision,avg_train_recall,avg_train_f1,avg_train_auc],[avg_test_accuracy,avg_test_precision,avg_test_recall,avg_test_f1,avg_test_auc]]\n",
    "report3=pd.DataFrame(data=stats3,index=['training set','test set'],columns=['accuracy','precision','recall','F1-score','AUC'])\n",
    "print('The average statistics over 30 simulations for Spectral Clustering:')\n",
    "report3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for the last simulation, report the ROC and the confusion matrix\n",
    "train_fpr,train_tpr,train_thresholds=roc_curve(Y_train,train_pred)\n",
    "test_fpr,test_tpr,test_thresholds=roc_curve(Y_test,test_pred)\n",
    "plt.figure()\n",
    "plt.plot(train_fpr,train_tpr,label='Training ROC curve')\n",
    "plt.plot(test_fpr,test_tpr,label='Testing ROC curve')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC for Spectral Cluster')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_cfm=confusion_matrix(Y_train,train_pred)\n",
    "test_cfm=confusion_matrix(Y_test,test_pred)\n",
    "print('Confusion matrix for training set:')\n",
    "print(train_cfm)\n",
    "print('Confusion matrix for testing set:')\n",
    "print(test_cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. One can expect that supervised learning on the full data set works better than semi-supervised learning with half of the data set labeled. \n",
    "One can expect that unsupervised learning underperforms in such situations. Compare the results you obtained by those methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the statistics after each learning method, it is obvious that supervised learning performs better than semi-supervised learning, \n",
    "but the gap is slight. Both supervised learning and semi-supervised learning performs way better than any one of the 2 unsupervised learning \n",
    "methods in spite of the similar performance on precision. Besides, each one has its own advantages between 2 unsupervised learning methods, \n",
    "k-means performs better based on precision and AUC, Spectral Clustering performs better based on accuracy and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Active Learning Using Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>(a) Download the banknote authentication Data Set from: https://archive.ics.uci.edu/ml/datasets/banknote+authentication. \n",
    "Choose 472 data points randomly as the test set, and the remaining 900 points as the training set. This is a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.66610</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.16740</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.63830</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.52280</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.45520</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>0.40614</td>\n",
       "      <td>1.34920</td>\n",
       "      <td>-1.4501</td>\n",
       "      <td>-0.55949</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1368</th>\n",
       "      <td>-1.38870</td>\n",
       "      <td>-4.87730</td>\n",
       "      <td>6.4774</td>\n",
       "      <td>0.34179</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>-3.75030</td>\n",
       "      <td>-13.45860</td>\n",
       "      <td>17.5932</td>\n",
       "      <td>-2.77710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1370</th>\n",
       "      <td>-3.56370</td>\n",
       "      <td>-8.38270</td>\n",
       "      <td>12.3930</td>\n",
       "      <td>-1.28230</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>-2.54190</td>\n",
       "      <td>-0.65804</td>\n",
       "      <td>2.6842</td>\n",
       "      <td>1.19520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1372 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy  class\n",
       "0      3.62160   8.66610   -2.8073 -0.44699      0\n",
       "1      4.54590   8.16740   -2.4586 -1.46210      0\n",
       "2      3.86600  -2.63830    1.9242  0.10645      0\n",
       "3      3.45660   9.52280   -4.0112 -3.59440      0\n",
       "4      0.32924  -4.45520    4.5718 -0.98880      0\n",
       "...        ...       ...       ...      ...    ...\n",
       "1367   0.40614   1.34920   -1.4501 -0.55949      1\n",
       "1368  -1.38870  -4.87730    6.4774  0.34179      1\n",
       "1369  -3.75030 -13.45860   17.5932 -2.77710      1\n",
       "1370  -3.56370  -8.38270   12.3930 -1.28230      1\n",
       "1371  -2.54190  -0.65804    2.6842  1.19520      1\n",
       "\n",
       "[1372 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=pd.read_csv('../data/data_banknote_authentication.txt',header=None)\n",
    "columns2=['variance','skewness','curtosis','entropy','class']\n",
    "data2.columns=columns2\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variance</th>\n",
       "      <th>skewness</th>\n",
       "      <th>curtosis</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>-0.40804</td>\n",
       "      <td>0.54214</td>\n",
       "      <td>-0.52725</td>\n",
       "      <td>0.65860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>1.60200</td>\n",
       "      <td>6.12510</td>\n",
       "      <td>0.52924</td>\n",
       "      <td>0.47886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>3.70220</td>\n",
       "      <td>6.99420</td>\n",
       "      <td>-1.85110</td>\n",
       "      <td>-0.12889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>-2.14050</td>\n",
       "      <td>-0.16762</td>\n",
       "      <td>1.32100</td>\n",
       "      <td>-0.20906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>4.92940</td>\n",
       "      <td>0.27727</td>\n",
       "      <td>0.20792</td>\n",
       "      <td>0.33662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>-2.66490</td>\n",
       "      <td>-12.81300</td>\n",
       "      <td>12.66890</td>\n",
       "      <td>-1.90820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.64472</td>\n",
       "      <td>-4.60620</td>\n",
       "      <td>8.34700</td>\n",
       "      <td>-2.70990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>4.33650</td>\n",
       "      <td>-3.58400</td>\n",
       "      <td>3.68840</td>\n",
       "      <td>0.74912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>0.74307</td>\n",
       "      <td>11.17000</td>\n",
       "      <td>-1.38240</td>\n",
       "      <td>-4.07280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059</th>\n",
       "      <td>-3.23050</td>\n",
       "      <td>-7.21350</td>\n",
       "      <td>11.64330</td>\n",
       "      <td>-0.94613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      variance  skewness  curtosis  entropy\n",
       "1018  -0.40804   0.54214  -0.52725  0.65860\n",
       "454    1.60200   6.12510   0.52924  0.47886\n",
       "704    3.70220   6.99420  -1.85110 -0.12889\n",
       "846   -2.14050  -0.16762   1.32100 -0.20906\n",
       "503    4.92940   0.27727   0.20792  0.33662\n",
       "...        ...       ...       ...      ...\n",
       "1085  -2.66490 -12.81300  12.66890 -1.90820\n",
       "95    -0.64472  -4.60620   8.34700 -2.70990\n",
       "683    4.33650  -3.58400   3.68840  0.74912\n",
       "372    0.74307  11.17000  -1.38240 -4.07280\n",
       "1059  -3.23050  -7.21350  11.64330 -0.94613\n",
       "\n",
       "[900 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_set=data2.iloc[:,:4]\n",
    "y_set=data2.iloc[:,-1]\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_set,y_set,test_size=472/(900+472),random_state=55,stratify=y_set)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Repeat each of the following two procedures 50 times. You will have 50 errors for 90 SVMs per each procedure.\n",
    "<br>i. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the \n",
    "penalty parameter using 5-fold cross validation. Repeat this process by adding 10 other randomly selected data points to the pool, until you \n",
    "use all the 900 points. Do NOT replace the samples back into the training set at each step. Calculate the test error for each SVM. You will \n",
    "have 90 SVMs that were trained using 10, 20, 30, ... , 900 data points and their 90 test errors. You have implemented passive learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def select_data():\n",
    "    IDs=[]\n",
    "    ranges=list(range(900))\n",
    "    ID0=random.sample(ranges,10)\n",
    "    sum0=sum(y_train.iloc[ID0])\n",
    "    while sum0<2 or sum0>9: # ensure both classes have at least 2 candidates in the first step\n",
    "        ID0=random.sample(ranges,10)\n",
    "        sum0=sum(y_train.iloc[ID0])\n",
    "    IDs.append(ID0)\n",
    "    for k in ID0:\n",
    "        ranges.remove(k)\n",
    "    while len(ranges)>0:\n",
    "        ID=random.sample(ranges,10)\n",
    "        IDs.append(ID)\n",
    "        for j in ID:\n",
    "            ranges.remove(j)\n",
    "    return IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 1 completed\n",
      "i= 2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 3 completed\n",
      "i= 4 completed\n",
      "i= 5 completed\n",
      "i= 6 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 7 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 8 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 9 completed\n",
      "i= 10 completed\n",
      "i= 11 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 12 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 13 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 14 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 15 completed\n",
      "i= 16 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 17 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 18 completed\n",
      "i= 19 completed\n",
      "i= 20 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 21 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 22 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 23 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 24 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 25 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 26 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 27 completed\n",
      "i= 28 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 29 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 30 completed\n",
      "i= 31 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 32 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 33 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 34 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 35 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 36 completed\n",
      "i= 37 completed\n",
      "i= 38 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 39 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 40 completed\n",
      "i= 41 completed\n",
      "i= 42 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 43 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 44 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "8 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "8 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1143, in _fit_liblinear\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 45 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 46 completed\n",
      "i= 47 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 48 completed\n",
      "i= 49 completed\n"
     ]
    }
   ],
   "source": [
    "Cs=[]\n",
    "log_c=range(-3,5)\n",
    "for l in log_c:\n",
    "    Cs.append(10**l)\n",
    "Error1=[]\n",
    "for i in range(50):\n",
    "    IDs=select_data()\n",
    "    errors=[]\n",
    "    x_training=pd.DataFrame()\n",
    "    y_training=[]\n",
    "    for j in range(90):\n",
    "        x_training=x_training.append(x_train.iloc[IDs[j]])\n",
    "        for k in IDs[j]:\n",
    "            y_training.append(y_train.iloc[k])\n",
    "        parameters={'C': Cs}\n",
    "        svc=LinearSVC(random_state=22,penalty='l1',dual=False,max_iter=1000000)\n",
    "        GS=GridSearchCV(svc,parameters,cv=5,n_jobs=-1).fit(x_training,y_training)\n",
    "        c=GS.best_params_['C'] # best c from cross validation\n",
    "        lsvc=LinearSVC(random_state=22, penalty='l1',dual=False,C=c,max_iter=1000000) \n",
    "        lsvc.fit(x_training,y_training)\n",
    "        errors.append(1-lsvc.score(x_test,y_test))\n",
    "    Error1.append(errors)\n",
    "    print('i=',i,'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.298729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.184322</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.243644</td>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.144068</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.277542</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.163136</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.207627</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.103814</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.141949</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.182203</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.444915</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.110169</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.046610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.122881</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.105932</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.173729  0.036017  0.031780  0.029661  0.010593  0.010593  0.019068   \n",
       "1   0.298729  0.048729  0.010593  0.012712  0.014831  0.014831  0.014831   \n",
       "2   0.175847  0.139831  0.033898  0.029661  0.021186  0.012712  0.012712   \n",
       "3   0.095339  0.029661  0.048729  0.027542  0.014831  0.016949  0.042373   \n",
       "4   0.184322  0.078390  0.078390  0.072034  0.069915  0.016949  0.019068   \n",
       "5   0.105932  0.012712  0.016949  0.019068  0.023305  0.027542  0.027542   \n",
       "6   0.052966  0.069915  0.069915  0.069915  0.027542  0.027542  0.023305   \n",
       "7   0.103814  0.031780  0.031780  0.029661  0.019068  0.012712  0.029661   \n",
       "8   0.074153  0.086864  0.059322  0.010593  0.019068  0.016949  0.025424   \n",
       "9   0.029661  0.044492  0.019068  0.010593  0.010593  0.016949  0.012712   \n",
       "10  0.243644  0.129237  0.120763  0.112288  0.023305  0.029661  0.027542   \n",
       "11  0.211864  0.031780  0.033898  0.038136  0.033898  0.031780  0.031780   \n",
       "12  0.169492  0.031780  0.012712  0.012712  0.031780  0.076271  0.031780   \n",
       "13  0.012712  0.023305  0.016949  0.019068  0.019068  0.006356  0.014831   \n",
       "14  0.029661  0.029661  0.010593  0.021186  0.016949  0.016949  0.012712   \n",
       "15  0.069915  0.044492  0.038136  0.012712  0.006356  0.008475  0.021186   \n",
       "16  0.029661  0.027542  0.038136  0.012712  0.012712  0.019068  0.021186   \n",
       "17  0.144068  0.067797  0.065678  0.027542  0.008475  0.008475  0.008475   \n",
       "18  0.305085  0.069915  0.072034  0.027542  0.025424  0.048729  0.031780   \n",
       "19  0.277542  0.175847  0.029661  0.033898  0.055085  0.050847  0.046610   \n",
       "20  0.116525  0.152542  0.163136  0.074153  0.029661  0.029661  0.050847   \n",
       "21  0.125000  0.046610  0.008475  0.014831  0.014831  0.019068  0.019068   \n",
       "22  0.207627  0.076271  0.078390  0.038136  0.038136  0.038136  0.016949   \n",
       "23  0.173729  0.203390  0.027542  0.027542  0.021186  0.027542  0.031780   \n",
       "24  0.027542  0.036017  0.074153  0.052966  0.036017  0.038136  0.021186   \n",
       "25  0.129237  0.088983  0.080508  0.050847  0.012712  0.012712  0.012712   \n",
       "26  0.040254  0.055085  0.061441  0.036017  0.029661  0.029661  0.012712   \n",
       "27  0.177966  0.074153  0.074153  0.031780  0.031780  0.031780  0.021186   \n",
       "28  0.031780  0.031780  0.019068  0.010593  0.008475  0.014831  0.014831   \n",
       "29  0.127119  0.065678  0.014831  0.012712  0.010593  0.010593  0.025424   \n",
       "30  0.173729  0.076271  0.036017  0.038136  0.019068  0.014831  0.019068   \n",
       "31  0.141949  0.084746  0.088983  0.069915  0.019068  0.023305  0.019068   \n",
       "32  0.125000  0.033898  0.033898  0.029661  0.025424  0.023305  0.023305   \n",
       "33  0.036017  0.016949  0.055085  0.008475  0.006356  0.006356  0.016949   \n",
       "34  0.091102  0.036017  0.029661  0.050847  0.048729  0.048729  0.023305   \n",
       "35  0.175847  0.040254  0.010593  0.010593  0.010593  0.010593  0.010593   \n",
       "36  0.103814  0.088983  0.031780  0.031780  0.014831  0.012712  0.014831   \n",
       "37  0.050847  0.029661  0.082627  0.059322  0.027542  0.027542  0.029661   \n",
       "38  0.133475  0.074153  0.033898  0.016949  0.014831  0.010593  0.014831   \n",
       "39  0.084746  0.042373  0.031780  0.055085  0.040254  0.019068  0.040254   \n",
       "40  0.148305  0.141949  0.069915  0.008475  0.008475  0.014831  0.010593   \n",
       "41  0.175847  0.175847  0.156780  0.044492  0.014831  0.014831  0.014831   \n",
       "42  0.012712  0.097458  0.116525  0.019068  0.031780  0.021186  0.008475   \n",
       "43  0.161017  0.033898  0.019068  0.033898  0.010593  0.010593  0.010593   \n",
       "44  0.182203  0.072034  0.042373  0.042373  0.029661  0.029661  0.023305   \n",
       "45  0.444915  0.061441  0.050847  0.038136  0.031780  0.038136  0.029661   \n",
       "46  0.110169  0.131356  0.069915  0.069915  0.055085  0.042373  0.029661   \n",
       "47  0.057203  0.025424  0.012712  0.012712  0.019068  0.016949  0.012712   \n",
       "48  0.122881  0.133475  0.021186  0.014831  0.006356  0.010593  0.031780   \n",
       "49  0.105932  0.076271  0.061441  0.042373  0.038136  0.027542  0.027542   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.010593  0.016949  0.008475  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "1   0.016949  0.016949  0.008475  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "2   0.010593  0.010593  0.010593  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "3   0.016949  0.038136  0.021186  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "4   0.021186  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "5   0.029661  0.029661  0.050847  ...  0.012712  0.019068  0.019068  0.016949   \n",
       "6   0.023305  0.025424  0.023305  ...  0.014831  0.014831  0.014831  0.016949   \n",
       "7   0.012712  0.014831  0.014831  ...  0.019068  0.016949  0.016949  0.016949   \n",
       "8   0.021186  0.029661  0.029661  ...  0.016949  0.021186  0.021186  0.016949   \n",
       "9   0.012712  0.019068  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "10  0.012712  0.027542  0.006356  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "11  0.029661  0.059322  0.057203  ...  0.016949  0.014831  0.014831  0.016949   \n",
       "12  0.033898  0.025424  0.025424  ...  0.010593  0.010593  0.014831  0.010593   \n",
       "13  0.010593  0.010593  0.010593  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "14  0.021186  0.021186  0.014831  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "15  0.014831  0.019068  0.014831  ...  0.016949  0.016949  0.019068  0.019068   \n",
       "16  0.006356  0.006356  0.008475  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "17  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "18  0.031780  0.023305  0.023305  ...  0.016949  0.019068  0.019068  0.019068   \n",
       "19  0.023305  0.021186  0.025424  ...  0.014831  0.016949  0.016949  0.016949   \n",
       "20  0.050847  0.033898  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "21  0.014831  0.019068  0.031780  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "22  0.016949  0.023305  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "23  0.023305  0.031780  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "24  0.019068  0.031780  0.027542  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "25  0.012712  0.012712  0.019068  ...  0.019068  0.019068  0.014831  0.014831   \n",
       "26  0.042373  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "27  0.025424  0.014831  0.014831  ...  0.021186  0.016949  0.016949  0.016949   \n",
       "28  0.014831  0.014831  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "29  0.031780  0.016949  0.031780  ...  0.019068  0.019068  0.019068  0.019068   \n",
       "30  0.014831  0.010593  0.010593  ...  0.016949  0.016949  0.014831  0.014831   \n",
       "31  0.019068  0.014831  0.019068  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "32  0.025424  0.025424  0.025424  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "33  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "34  0.012712  0.012712  0.012712  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "35  0.008475  0.008475  0.014831  ...  0.014831  0.016949  0.016949  0.016949   \n",
       "36  0.014831  0.016949  0.019068  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "37  0.044492  0.038136  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "38  0.014831  0.014831  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "39  0.044492  0.044492  0.042373  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "40  0.010593  0.010593  0.010593  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "41  0.014831  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "42  0.008475  0.010593  0.006356  ...  0.008475  0.008475  0.016949  0.010593   \n",
       "43  0.010593  0.014831  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "44  0.021186  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "45  0.031780  0.031780  0.023305  ...  0.014831  0.014831  0.016949  0.016949   \n",
       "46  0.046610  0.021186  0.046610  ...  0.021186  0.021186  0.019068  0.019068   \n",
       "47  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "48  0.031780  0.021186  0.021186  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "49  0.016949  0.019068  0.014831  ...  0.014831  0.019068  0.019068  0.019068   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.014831  0.016949  0.016949  0.014831  0.016949  0.016949  \n",
       "1   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "2   0.016949  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "3   0.014831  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "4   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "5   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "6   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "7   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "8   0.016949  0.016949  0.021186  0.016949  0.016949  0.016949  \n",
       "9   0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "10  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "11  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "12  0.014831  0.014831  0.014831  0.014831  0.016949  0.016949  \n",
       "13  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "14  0.016949  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "15  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "16  0.014831  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "17  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "18  0.019068  0.019068  0.016949  0.016949  0.016949  0.016949  \n",
       "19  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "20  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "21  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "22  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "23  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "24  0.014831  0.014831  0.014831  0.014831  0.016949  0.016949  \n",
       "25  0.014831  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "26  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "27  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "28  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "29  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "30  0.016949  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "31  0.014831  0.014831  0.014831  0.016949  0.016949  0.016949  \n",
       "32  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "33  0.016949  0.016949  0.014831  0.014831  0.016949  0.016949  \n",
       "34  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "35  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "36  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "37  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "38  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "39  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "40  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "41  0.016949  0.016949  0.014831  0.016949  0.016949  0.016949  \n",
       "42  0.021186  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "43  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "44  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "45  0.016949  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "46  0.016949  0.016949  0.014831  0.016949  0.010593  0.010593  \n",
       "47  0.014831  0.014831  0.016949  0.016949  0.016949  0.016949  \n",
       "48  0.016949  0.016949  0.016949  0.016949  0.019068  0.016949  \n",
       "49  0.019068  0.016949  0.016949  0.016949  0.016949  0.016949  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Error1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a SVM with a pool of 10 randomly selected data points from the training set using linear kernel and L1 penalty. Select the \n",
    "parameters of the SVM with 5-fold cross validation. Choose the 10 closest data points in the training set to the hyperplane of the SVM and \n",
    "add them to the pool. Do not replace the samples back into the training set. Train a new SVM using the pool. Repeat this process until all \n",
    "training data is used. You will have 90 SVMs that were trained using 10, 20, 30,..., 900 data points and their 90 test errors. You have \n",
    "implemented active learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data():\n",
    "    ranges=list(range(900))\n",
    "    ID0=random.sample(ranges,10)\n",
    "    sum0=sum(y_train.iloc[ID0])\n",
    "    while sum0<2 or sum0>9: # ensure both classes have at least 2 candidates in the first step\n",
    "        ID0=random.sample(ranges,10)\n",
    "        sum0=sum(y_train.iloc[ID0])\n",
    "    return ID0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 0 completed\n",
      "i= 1 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 2 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 3 completed\n",
      "i= 4 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 5 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 6 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 7 completed\n",
      "i= 8 completed\n",
      "i= 9 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 10 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 11 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 12 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 13 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 14 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 15 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 16 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 17 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 18 completed\n",
      "i= 19 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 20 completed\n",
      "i= 21 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 22 completed\n",
      "i= 23 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 24 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 25 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 26 completed\n",
      "i= 27 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 28 completed\n",
      "i= 29 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 30 completed\n",
      "i= 31 completed\n",
      "i= 32 completed\n",
      "i= 33 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 34 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 35 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 36 completed\n",
      "i= 37 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 38 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 39 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 40 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 41 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 42 completed\n",
      "i= 43 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 44 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 45 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 46 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 47 completed\n",
      "i= 48 completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\usc\\anaconda\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i= 49 completed\n"
     ]
    }
   ],
   "source": [
    "Error2=[]\n",
    "for i in range(50):\n",
    "    ID0=initial_data()\n",
    "    x_training=x_train.iloc[ID0]\n",
    "    y_training=y_train.iloc[ID0]       \n",
    "    x_remain=x_train.drop(x_train.index[ID0])\n",
    "    y_remain=y_train.drop(y_train.index[ID0])\n",
    "    x_training=x_training.reset_index(drop=True)\n",
    "    y_training=y_training.reset_index(drop=True)\n",
    "    x_remain=x_remain.reset_index(drop=True)\n",
    "    y_remain=y_remain.reset_index(drop=True)\n",
    "    errors = list()\n",
    "    for j in range(90):\n",
    "        parameters = {'C':Cs}\n",
    "        svc=LinearSVC(random_state=22,penalty='l1',dual=False,max_iter=1000000)\n",
    "        GS=GridSearchCV(svc,parameters,cv=5,n_jobs=-1).fit(x_training,y_training)\n",
    "        c=GS.best_params_['C'] # best c from cross validation\n",
    "        lsvc=LinearSVC(random_state=22, penalty='l1',dual=False,C=c,max_iter=1000000) \n",
    "        lsvc.fit(x_training,y_training)\n",
    "        errors.append(1-lsvc.score(x_test,y_test))\n",
    "        if j<89:\n",
    "            distances=lsvc.decision_function(x_remain)\n",
    "            ID=np.argsort(abs(distances))[:10] # 10 closest data points in the remaining set\n",
    "            x_training=pd.concat([x_training,x_remain.iloc[ID]],axis=0,ignore_index=True)\n",
    "            y_training=pd.concat([y_training,y_remain.iloc[ID]],axis=0,ignore_index=True)\n",
    "            x_remain=x_remain.drop(x_remain.index[ID])\n",
    "            y_remain=y_remain.drop(y_remain.index[ID])\n",
    "            x_remain=x_remain.reset_index(drop=True)\n",
    "            y_remain=y_remain.reset_index(drop=True)\n",
    "    Error2.append(errors)\n",
    "    print('i=',i,'completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.082627</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.211864</td>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.296610</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.156780</td>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.139831</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.097458</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.095339</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.063559</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.177966</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.148305</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.167373</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.131356</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.120763</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.173729</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.161017</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.281780</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.129237</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.190678</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.080508</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.093220</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.116525</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.069915</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.074153</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.175847</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.133475</td>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.055085</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.044492</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.048729</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.158898</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.108051</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.065678</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.114407</td>\n",
       "      <td>0.213983</td>\n",
       "      <td>0.061441</td>\n",
       "      <td>0.038136</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.031780</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.088983</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.300847</td>\n",
       "      <td>0.078390</td>\n",
       "      <td>0.052966</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.228814</td>\n",
       "      <td>0.086864</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.076271</td>\n",
       "      <td>0.059322</td>\n",
       "      <td>0.042373</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.099576</td>\n",
       "      <td>0.072034</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "      <td>0.010593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.254237</td>\n",
       "      <td>0.057203</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.040254</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.019068</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "      <td>0.014831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 90 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.139831  0.044492  0.023305  0.014831  0.036017  0.021186  0.016949   \n",
       "1   0.158898  0.025424  0.006356  0.014831  0.004237  0.012712  0.014831   \n",
       "2   0.082627  0.074153  0.052966  0.027542  0.027542  0.027542  0.027542   \n",
       "3   0.091102  0.044492  0.023305  0.012712  0.019068  0.014831  0.021186   \n",
       "4   0.033898  0.025424  0.012712  0.004237  0.006356  0.019068  0.016949   \n",
       "5   0.139831  0.029661  0.019068  0.040254  0.019068  0.019068  0.016949   \n",
       "6   0.050847  0.027542  0.014831  0.084746  0.004237  0.006356  0.016949   \n",
       "7   0.175847  0.021186  0.023305  0.010593  0.069915  0.021186  0.021186   \n",
       "8   0.080508  0.031780  0.033898  0.021186  0.019068  0.016949  0.014831   \n",
       "9   0.078390  0.076271  0.027542  0.010593  0.012712  0.008475  0.021186   \n",
       "10  0.211864  0.080508  0.048729  0.296610  0.031780  0.027542  0.023305   \n",
       "11  0.156780  0.133475  0.014831  0.048729  0.008475  0.010593  0.016949   \n",
       "12  0.139831  0.029661  0.025424  0.048729  0.012712  0.006356  0.025424   \n",
       "13  0.097458  0.057203  0.040254  0.021186  0.021186  0.019068  0.016949   \n",
       "14  0.031780  0.069915  0.019068  0.036017  0.023305  0.023305  0.016949   \n",
       "15  0.095339  0.025424  0.063559  0.040254  0.016949  0.014831  0.016949   \n",
       "16  0.173729  0.016949  0.027542  0.014831  0.008475  0.012712  0.016949   \n",
       "17  0.177966  0.021186  0.084746  0.010593  0.010593  0.016949  0.016949   \n",
       "18  0.148305  0.027542  0.036017  0.025424  0.027542  0.010593  0.014831   \n",
       "19  0.167373  0.029661  0.031780  0.078390  0.008475  0.044492  0.029661   \n",
       "20  0.120763  0.131356  0.072034  0.019068  0.031780  0.033898  0.027542   \n",
       "21  0.120763  0.036017  0.008475  0.074153  0.016949  0.019068  0.021186   \n",
       "22  0.067797  0.012712  0.114407  0.012712  0.012712  0.010593  0.016949   \n",
       "23  0.173729  0.025424  0.016949  0.006356  0.010593  0.023305  0.016949   \n",
       "24  0.161017  0.169492  0.055085  0.036017  0.040254  0.029661  0.033898   \n",
       "25  0.186441  0.016949  0.016949  0.004237  0.004237  0.012712  0.016949   \n",
       "26  0.084746  0.044492  0.029661  0.016949  0.021186  0.016949  0.016949   \n",
       "27  0.281780  0.190678  0.031780  0.031780  0.023305  0.012712  0.016949   \n",
       "28  0.129237  0.067797  0.021186  0.027542  0.033898  0.036017  0.023305   \n",
       "29  0.080508  0.190678  0.044492  0.027542  0.040254  0.038136  0.023305   \n",
       "30  0.133475  0.086864  0.008475  0.023305  0.004237  0.016949  0.019068   \n",
       "31  0.125000  0.052966  0.012712  0.033898  0.025424  0.025424  0.029661   \n",
       "32  0.080508  0.023305  0.019068  0.021186  0.012712  0.014831  0.016949   \n",
       "33  0.125000  0.093220  0.048729  0.116525  0.048729  0.031780  0.027542   \n",
       "34  0.019068  0.069915  0.023305  0.012712  0.025424  0.019068  0.016949   \n",
       "35  0.016949  0.023305  0.074153  0.012712  0.019068  0.016949  0.016949   \n",
       "36  0.061441  0.175847  0.036017  0.014831  0.016949  0.014831  0.016949   \n",
       "37  0.078390  0.088983  0.048729  0.025424  0.021186  0.016949  0.016949   \n",
       "38  0.133475  0.044492  0.055085  0.052966  0.016949  0.016949  0.023305   \n",
       "39  0.044492  0.078390  0.038136  0.036017  0.033898  0.048729  0.023305   \n",
       "40  0.158898  0.029661  0.036017  0.010593  0.012712  0.014831  0.023305   \n",
       "41  0.108051  0.067797  0.012712  0.065678  0.019068  0.004237  0.004237   \n",
       "42  0.114407  0.213983  0.061441  0.038136  0.012712  0.016949  0.021186   \n",
       "43  0.254237  0.052966  0.031780  0.006356  0.004237  0.021186  0.014831   \n",
       "44  0.088983  0.027542  0.012712  0.014831  0.016949  0.016949  0.016949   \n",
       "45  0.300847  0.078390  0.052966  0.029661  0.025424  0.019068  0.016949   \n",
       "46  0.228814  0.086864  0.101695  0.076271  0.059322  0.042373  0.036017   \n",
       "47  0.099576  0.072034  0.012712  0.004237  0.004237  0.004237  0.023305   \n",
       "48  0.021186  0.027542  0.021186  0.023305  0.016949  0.016949  0.016949   \n",
       "49  0.254237  0.057203  0.040254  0.027542  0.040254  0.033898  0.019068   \n",
       "\n",
       "          7         8         9   ...        80        81        82        83  \\\n",
       "0   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "1   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "2   0.014831  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "3   0.014831  0.016949  0.010593  ...  0.016949  0.016949  0.014831  0.014831   \n",
       "4   0.016949  0.016949  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "5   0.016949  0.016949  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "6   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "7   0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "8   0.016949  0.016949  0.016949  ...  0.010593  0.014831  0.014831  0.014831   \n",
       "9   0.019068  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "10  0.014831  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "11  0.014831  0.016949  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "12  0.023305  0.016949  0.016949  ...  0.016949  0.016949  0.016949  0.010593   \n",
       "13  0.016949  0.016949  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "14  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "15  0.016949  0.016949  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "16  0.014831  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "17  0.016949  0.016949  0.008475  ...  0.016949  0.010593  0.010593  0.010593   \n",
       "18  0.016949  0.016949  0.016949  ...  0.010593  0.010593  0.014831  0.014831   \n",
       "19  0.027542  0.021186  0.012712  ...  0.016949  0.016949  0.014831  0.014831   \n",
       "20  0.019068  0.016949  0.008475  ...  0.014831  0.014831  0.010593  0.010593   \n",
       "21  0.014831  0.008475  0.016949  ...  0.014831  0.014831  0.010593  0.010593   \n",
       "22  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "23  0.016949  0.016949  0.016949  ...  0.016949  0.010593  0.010593  0.010593   \n",
       "24  0.023305  0.021186  0.016949  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "25  0.016949  0.010593  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "26  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "27  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "28  0.021186  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "29  0.019068  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "30  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "31  0.016949  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "32  0.016949  0.016949  0.008475  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "33  0.019068  0.023305  0.012712  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "34  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "35  0.016949  0.010593  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "36  0.016949  0.016949  0.016949  ...  0.016949  0.014831  0.014831  0.010593   \n",
       "37  0.016949  0.016949  0.006356  ...  0.016949  0.016949  0.016949  0.014831   \n",
       "38  0.016949  0.016949  0.016949  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "39  0.019068  0.014831  0.014831  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "40  0.016949  0.014831  0.016949  ...  0.014831  0.016949  0.014831  0.016949   \n",
       "41  0.023305  0.019068  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "42  0.014831  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "43  0.016949  0.016949  0.008475  ...  0.016949  0.014831  0.014831  0.014831   \n",
       "44  0.016949  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "45  0.008475  0.016949  0.010593  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "46  0.027542  0.033898  0.014831  ...  0.016949  0.016949  0.016949  0.016949   \n",
       "47  0.019068  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "48  0.016949  0.016949  0.008475  ...  0.010593  0.010593  0.010593  0.010593   \n",
       "49  0.014831  0.016949  0.016949  ...  0.014831  0.014831  0.014831  0.014831   \n",
       "\n",
       "          84        85        86        87        88        89  \n",
       "0   0.014831  0.014831  0.010593  0.014831  0.014831  0.014831  \n",
       "1   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "2   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "3   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "4   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "5   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "6   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "7   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "8   0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "9   0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "10  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "11  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "12  0.010593  0.010593  0.010593  0.016949  0.016949  0.016949  \n",
       "13  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "14  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "15  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "16  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "17  0.010593  0.010593  0.010593  0.014831  0.014831  0.014831  \n",
       "18  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "19  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "20  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "21  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "22  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "23  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "24  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "25  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "26  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "27  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "28  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "29  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "30  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "31  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "32  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "33  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "34  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "35  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "36  0.010593  0.010593  0.010593  0.010593  0.014831  0.014831  \n",
       "37  0.010593  0.010593  0.014831  0.014831  0.014831  0.014831  \n",
       "38  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "39  0.014831  0.014831  0.014831  0.014831  0.010593  0.010593  \n",
       "40  0.016949  0.010593  0.014831  0.010593  0.014831  0.016949  \n",
       "41  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "42  0.014831  0.014831  0.014831  0.010593  0.010593  0.014831  \n",
       "43  0.014831  0.014831  0.014831  0.010593  0.010593  0.010593  \n",
       "44  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "45  0.016949  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "46  0.016949  0.016949  0.016949  0.016949  0.014831  0.016949  \n",
       "47  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "48  0.010593  0.010593  0.010593  0.010593  0.010593  0.010593  \n",
       "49  0.014831  0.014831  0.014831  0.014831  0.014831  0.014831  \n",
       "\n",
       "[50 rows x 90 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(Error2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Average the 50 test errors for each of the incrementally trained 90 SVMs in 2(b)i and 2(b)ii. By doing so, you are performing a Monte \n",
    "Carlo simulation. Plot average test error versus number of training instances for both active and passive learners on the same figure and \n",
    "report your conclusions. Here, you are actually obtaining a learning curve by Monte-Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(0,900,10), pd.DataFrame(Error1).mean(), c='r', label='Passive Learning')\n",
    "plt.plot(range(0,900,10), pd.DataFrame(Error2).mean(), c='b', label='Active Learning')\n",
    "plt.xlabel('number of training instances')\n",
    "plt.ylabel('test error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: It is obvious that the test error of active learning decreases much faster and achieves a lower level than the test error \n",
    "of passive learning as the number of training instances increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
